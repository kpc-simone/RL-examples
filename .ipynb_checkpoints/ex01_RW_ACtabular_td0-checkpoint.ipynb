{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fda2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self,\n",
    "            n_states_ = 100,\n",
    "            states_reward_ = [10,50],\n",
    "            state_start_ = 25,\n",
    "            max_steps_ = 100\n",
    "            ):\n",
    "            \n",
    "        self.n_states = n_states_\n",
    "        self.reward_states = states_reward_\n",
    "        \n",
    "        self.state_start = np.random.randint(low = 0, high = self.n_states-1)\n",
    "        self.state = self.state_start\n",
    "        self.next_state = self.state_start\n",
    "        self.reward_counter = 0\n",
    "        self.step_count = 0\n",
    "        self.max_steps = max_steps_\n",
    "            \n",
    "    def reset(self):\n",
    "        self.state_start = np.random.randint(low = 0, high = self.n_states-1)\n",
    "        self.state = self.state_start\n",
    "        self.next_state = self.state_start\n",
    "        self.done = False\n",
    "        self.reward = 0.\n",
    "        self.reward_counter = 0\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.step_count += 1\n",
    "\n",
    "        self.next_state = self.state + action\n",
    "        \n",
    "        if self.next_state == 0 or self.next_state == self.n_states-1:\n",
    "            self.done = True\n",
    "        elif self.reward_counter == 1:\n",
    "            self.done = True\n",
    "        elif self.next_state in self.reward_states:\n",
    "            self.reward = 1.0 / ( 0.5 * self.step_count )\n",
    "            self.reward_counter += 1\n",
    "        elif self.step_count > self.max_steps:\n",
    "            self.done = True\n",
    "        \n",
    "        return self.next_state,self.reward,self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f693ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACAgent:\n",
    "    def __init__(self,\n",
    "            n_states_ = 50,   \n",
    "            actions_ = [-1,0,1],     # move left, stay in place, move right\n",
    "            state_start_ = 25,\n",
    "            alpha_ = .1,             # learning rate\n",
    "            gamma_ = 1.,             # discount rate, in this case none\n",
    "            epsilon_ = 0.75          # probability of moving randomly\n",
    "            ):\n",
    "        \n",
    "        self.n_states = n_states_\n",
    "        self.actions = actions_\n",
    "        self.alpha = alpha_\n",
    "        self.gamma = gamma_\n",
    "        self.epsilon = epsilon_\n",
    "        self.choice = 0\n",
    "        self.values = np.random.uniform( low = 0., high = 1., size = self.n_states ) / 1000\n",
    "        self.policy = np.random.uniform( low = 0., high = 1., size = (self.n_states,len(self.actions)) ) / 1000\n",
    "        \n",
    "    ### CRITIC ###\n",
    "    def compute_td_error(self,state,next_state,reward):\n",
    "        # TD error is reward prediction error\n",
    "        self.td_error = reward + self.gamma * self.values[next_state] - self.values[state]\n",
    "        \n",
    "    def update_value(self,state,next_state,reward):\n",
    "        # update value estimate for this timestep by scaling TD error by learning rate\n",
    "        # if td_error is positive, increases value of state\n",
    "        # if td_error is negative, decreases value of state\n",
    " \n",
    "        self.compute_td_error(state,next_state,reward)\n",
    "        self.values[state] = self.values[state] + self.alpha * self.td_error\n",
    "    \n",
    "    ### ACTOR ###    \n",
    "    def select_action(self,state):\n",
    "        # select action to exploit (move in direction of highest predicted value, i.e. greedily)\n",
    "        # or explore (i.e. randomly)\n",
    "        \n",
    "        p = np.random.uniform( low = 0., high = 1.)\n",
    "        if p > self.epsilon:\n",
    "            self.choice = np.argmax(self.policy[state,:])\n",
    "            action = self.actions[ self.choice ]\n",
    "        else:\n",
    "            self.choice = np.random.randint(0,len(self.actions))\n",
    "            action = self.actions[self.choice]\n",
    "        return action\n",
    "        \n",
    "    def update_policy(self,state):\n",
    "        self.policy[state,self.choice] = self.policy[state,self.choice] + self.alpha * self.td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "# script parameters\n",
    "n_episodes = 2500          # set how many episodes\n",
    "n_states = 100             # set how many states in the environment\n",
    "actions = [-1,0,1]         # define the actions the agent can take\n",
    "                           # in this case, move left, stay in place, or move right\n",
    "# agent hyperparameters\n",
    "gamma = 1.                 # discount rate, none in this case\n",
    "alpha = .1                 # learning rate\n",
    "epsilon = .75              # probability of exploration in any one time step\n",
    "\n",
    "# environment parameters\n",
    "states_reward = [33,67]    # set where the rewards are in the environment\n",
    "state_start = np.random.randint(low = 0, high = n_states-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OBJECT CREATION ###\n",
    "\n",
    "# create the environment\n",
    "environment = Environment(n_states_ = n_states, \n",
    "                            states_reward_ = states_reward, \n",
    "                            state_start_ = state_start)\n",
    "\n",
    "# create the agent\n",
    "agent = ACAgent(n_states_ = n_states,\n",
    "                    gamma_ = gamma,\n",
    "                    alpha_ = alpha,\n",
    "                    epsilon_ = epsilon)\n",
    "\n",
    "# create structures for saving the training data\n",
    "import pandas as pd\n",
    "vdf = pd.DataFrame()                    # value function estimates per episode\n",
    "pdf = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041dc29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TRAINING LOOP ###\n",
    "\n",
    "for e in range(0,n_episodes):\n",
    "    environment.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(environment.state)\n",
    "        next_state,reward,done = environment.step(action)\n",
    "        \n",
    "        agent.update_value(environment.state,environment.next_state,reward)\n",
    "        agent.update_policy(environment.state)\n",
    "        environment.state = environment.next_state\n",
    "        \n",
    "    vdf[e] = agent.values\n",
    "    pdf[e] = [ agent.actions[i] for i in np.argmax(agent.policy,axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f37102",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VISUALIZATION ###\n",
    "import matplotlib.pyplot as plt\n",
    "fig,axes = plt.subplots(2,2,figsize=(7,7),gridspec_kw={'width_ratios':[50,1]})\n",
    "\n",
    "# plot learning of value function\n",
    "ax = axes[0,0]\n",
    "cax = axes[0,1]\n",
    "ax.set_title(r'$\\alpha$ = {}'.format(alpha))\n",
    "im = ax.imshow(vdf,aspect='auto',interpolation='none',vmin=0.,vmax=10.,cmap='inferno')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('State')\n",
    "ax.set_ylim(0,100)\n",
    "ticks = [0,5.,10.]\n",
    "cbar = fig.colorbar(im, cax = cax, ticks = ticks, orientation='vertical')  \n",
    "cbar.set_label('Value',va='top',ha='left',rotation=90,in_layout=True)\n",
    "\n",
    "for state_reward in states_reward:\n",
    "    ax.axhline(state_reward,color='dimgray',linestyle='--')\n",
    "\n",
    "# plot learning of policy\n",
    "ax = axes[1,0]\n",
    "cax = axes[1,1]\n",
    "im = ax.imshow(pdf,aspect='auto',interpolation='none',vmin=-1.,vmax=1.,cmap='viridis')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('State')\n",
    "ax.set_ylim(0,100)\n",
    "ticks = [-1,0,1]\n",
    "ticklabels = [r'$\\downarrow$',r'$\\times$',r'$\\uparrow$']\n",
    "cbar = fig.colorbar(im, cax = cax, ticks = ticks, orientation='vertical')  \n",
    "cbar.set_label('Policy',va='top',ha='left',rotation=90,in_layout=True)  \n",
    "cbar.ax.set_yticklabels(ticklabels)\n",
    "\n",
    "for state_reward in states_reward:\n",
    "    ax.axhline(state_reward,color='white',linestyle='--')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
